{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\dell\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (3.141.0)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\dell\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from selenium) (1.25.8)\n"
     ]
    }
   ],
   "source": [
    "# https://pypi.org/project/selenium/\n",
    "# https://selenium-python.readthedocs.io/api.html\n",
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipython in c:\\users\\dell\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (7.11.1)\n",
      "Requirement already satisfied: jedi>=0.10 in c:\\users\\dell\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from ipython) (0.16.0)\n",
      "Requirement already satisfied: colorama; sys_platform == \"win32\" in c:\\users\\dell\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from ipython) (0.4.3)\n",
      "Requirement already satisfied: backcall in c:\\users\\dell\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from ipython) (0.1.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in c:\\users\\dell\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from ipython) (4.3.3)\n",
      "Requirement already satisfied: pygments in c:\\users\\dell\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from ipython) (2.4.2)\n",
      "Requirement already satisfied: decorator in c:\\users\\dell\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from ipython) (4.4.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\dell\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from ipython) (3.0.3)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\dell\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from ipython) (0.7.5)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\dell\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from ipython) (45.1.0.post20200127)\n",
      "Requirement already satisfied: parso>=0.5.2 in c:\\users\\dell\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from jedi>=0.10->ipython) (0.6.2)\n",
      "Requirement already satisfied: six in c:\\users\\dell\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from traitlets>=4.2->ipython) (1.14.0)\n",
      "Requirement already satisfied: ipython-genutils in c:\\users\\dell\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from traitlets>=4.2->ipython) (0.2.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\dell\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython) (0.1.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install csv\n",
    "pip install time \n",
    "pip install parsel\n",
    "#pip install lxml\n",
    "pip install bs4\n",
    "pip install html5lib\n",
    "pip install et_xmlfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lxml\n",
    "import requests, time, random\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = webdriver.Chrome('chromedriver.exe')\n",
    "browser.get('https://www.linkedin.com/uas/login')\n",
    "file = open('config.txt')\n",
    "lines = file.readlines()\n",
    "username = lines[0]\n",
    "password = lines[1]\n",
    "\n",
    "\n",
    "elementID = browser.find_element_by_id('username')\n",
    "elementID.send_keys(username)\n",
    "\n",
    "elementID = browser.find_element_by_id('password')\n",
    "elementID.send_keys(password)\n",
    "\n",
    "#elementID.submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slicing fonction Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HTML_Sraping(link):\n",
    "   \n",
    "    browser.get(link)\n",
    "    res=requests.get(link)\n",
    "    SCROLL_PAUSE_TIME = 5\n",
    "# Get scroll height\n",
    "    y = 500\n",
    "    y1 = 500\n",
    "    # Get scroll height\n",
    "    z = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        #pass\n",
    "    for i in range(0,30):\n",
    "        # Scroll down to bottom\n",
    "        browser.execute_script(\"window.scrollTo(0, \"+str(y)+\")\")\n",
    "        y += random.randint(300, 600) \n",
    "        time.sleep(0.1)\n",
    "        # Wait to load page\n",
    "        #time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height <= y:\n",
    "            break\n",
    "    for timer in range(0,30):\n",
    "        browser.execute_script(\"window.scrollTo(0, \"+str(z-y1)+\")\")\n",
    "        y1 += random.randint(500, 600) \n",
    "        time.sleep(0.1)\n",
    "        if z <= y1/2:\n",
    "            break         \n",
    "    try:\n",
    "        browser.find_element_by_css_selector(\"button[data-control-name='skill_details']\").click() \n",
    "        #print(\"clicked\")\n",
    "    except:\n",
    "        pass\n",
    "    src = browser.page_source\n",
    "    #soup = BeautifulSoup(src, 'lxml')\n",
    "    soup = BeautifulSoup(src,'lxml')\n",
    "    #soup = src\n",
    "    return soup    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Json_Scraping_2(link):\n",
    "    soup=HTML_Sraping(link)\n",
    "    final_liste=[]\n",
    "    info=dict()\n",
    "    info[\"link\"]=[link]\n",
    "     ###########Personal info_name\n",
    "    try:\n",
    "        \n",
    "        #info ={'link':'Vide','fullName':'Vide','profile_title':'Vide','location':'Vide','connectionsCount':'Vide','job_title':'Vide','company_name':'Vide','joining_date':'Vide','exp_duree':'Vide','college_name':'Vide','degree_name':'Vide','stream':'Vide','degree_year':'Vide','Language':'Vide'}\n",
    "        name_div = soup.find('div', {'class': 'flex-1 mr5'})\n",
    "        name_loc = name_div.find_all('ul')\n",
    "        fullName = name_loc[0].find('li').get_text().strip()    \n",
    "        \n",
    "        info[\"fullName\"]=[fullName]\n",
    "    except:\n",
    "        #print('no data')\n",
    "\n",
    "        pass         \n",
    "         ###########Personal info_Location   \n",
    "    try:    \n",
    "        name_div = soup.find('div', {'class': 'flex-1 mr5'})\n",
    "        name_loc = name_div.find_all('ul')\n",
    "        location = name_loc[1].find('li').get_text().strip()\n",
    "        info[\"location\"]=[location]\n",
    "    except:\n",
    "        #print('no data')\n",
    "\n",
    "        pass \n",
    "         ###########Personal info_Profile_title\n",
    "    try:\n",
    "        name_div = soup.find('div', {'class': 'flex-1 mr5'})\n",
    "        profile_title = name_div.find('h2').get_text().strip()\n",
    "        info[\"profile_title\"]=[profile_title]\n",
    "    except:\n",
    "        #print('no data')\n",
    "\n",
    "        pass     \n",
    "\n",
    "          ###########Personal info_ConnectionsCount\n",
    "    try:\n",
    "        name_div = soup.find('div', {'class': 'flex-1 mr5'})\n",
    "        name_loc = name_div.find_all('ul')\n",
    "        name_loc = name_div.find_all('ul')\n",
    "        connection = name_loc[1].find_all('li')\n",
    "        connectionsCount = connection[1].get_text().strip()\n",
    "\n",
    "        info[\"connectionsCount\"]=[connectionsCount]\n",
    "    except:\n",
    "        #print('no data')\n",
    "\n",
    "        pass     \n",
    "\n",
    "    ############## Experience\n",
    "    try:\n",
    "        exp_section = soup.find('section', {'id': 'experience-section'}).find('ul')\n",
    "        job_title=[]\n",
    "        jo=[]\n",
    "        company_name=[]\n",
    "        joining_date=[]\n",
    "        exp_duree=[]\n",
    "        for i in range(len(exp_section.find_all('div', {'class': \"display-flex flex-column full-width\"}))):\n",
    "            a_tags=exp_section.find_all('div', {'class': \"display-flex flex-column full-width\"})[i].find('a')\n",
    "            jo.append(a_tags.find('h3').get_text().strip())\n",
    "            company_name .append( a_tags.find_all('p')[1].get_text().strip())\n",
    "            joining_date .append(a_tags.find_all('h4')[0].find_all('span')[1].get_text().strip())\n",
    "            exp_duree .append( a_tags.find_all('h4')[1].find_all('span')[1].get_text().strip())\n",
    "            info[\"job_title\"]=[jo]\n",
    "            info[\"company_name\"]=[company_name] \n",
    "            info[\"joining_date\"]=[joining_date]\n",
    "            info[\"exp_duree\"]=[exp_duree]\n",
    "    except:\n",
    "        #print('no data')\n",
    "        pass     \n",
    "\n",
    "    #############Education( college_name)\n",
    "    try:\n",
    "        edu_section = soup.find('section', {'id': 'education-section'}).find('ul')\n",
    "        college_name=[]\n",
    "\n",
    "        for i in range(len(edu_section.find_all('div', {'class':\"display-flex flex-column full-width\"}))):\n",
    "            edu=edu_section.find_all('div', {'class':\"display-flex flex-column full-width\"})[i]\n",
    "            college_name.append(edu.find('h3').get_text().strip())\n",
    "            info[\"college_name\"]=[college_name]\n",
    "\n",
    "    except:\n",
    "        #print('no data')\n",
    "        pass        \n",
    "    #############Education( degree_name)\n",
    "    try:\n",
    "        edu_section = soup.find('section', {'id': 'education-section'}).find('ul')\n",
    "    \n",
    "        degree_name=[]\n",
    "   \n",
    "        for i in range(len(edu_section.find_all('div', {'class':\"display-flex flex-column full-width\"}))):\n",
    "            edu=edu_section.find_all('div', {'class':\"display-flex flex-column full-width\"})[i]\n",
    "            \n",
    "            degree_name.append(edu.find('p', {'class': 'pv-entity__secondary-title pv-entity__degree-name t-14 t-black t-normal'}).find_all('span')[1].get_text().strip())\n",
    "           \n",
    "            info[\"degree_name\"]=[degree_name]\n",
    "            \n",
    "    except:\n",
    "        #print('no data')\n",
    "        pass\n",
    "    #############Education(stream)\n",
    "    try:\n",
    "        edu_section = soup.find('section', {'id': 'education-section'}).find('ul')\n",
    "        \n",
    "        stream =[]\n",
    "        degree_year=[]\n",
    "        for i in range(len(edu_section.find_all('div', {'class':\"display-flex flex-column full-width\"}))):\n",
    "            edu=edu_section.find_all('div', {'class':\"display-flex flex-column full-width\"})[i]\n",
    "            \n",
    "            stream.append(edu.find('p', {'class': 'pv-entity__secondary-title pv-entity__fos t-14 t-black t-normal'}).find_all('span')[1].get_text().strip()) \n",
    "            info[\"stream\"]=[stream]\n",
    "            \n",
    "    except:\n",
    "        #print('no data')\n",
    "        pass\n",
    "    #############Education(degree_year)\n",
    "    try:\n",
    "        edu_section = soup.find('section', {'id': 'education-section'}).find('ul')\n",
    "        degree_year=[]\n",
    "        for i in range(len(edu_section.find_all('div', {'class':\"display-flex flex-column full-width\"}))):\n",
    "            edu=edu_section.find_all('div', {'class':\"display-flex flex-column full-width\"})[i]\n",
    "            degree_year.append( edu.find('p', {'class': 'pv-entity__dates t-14 t-black--light t-normal'}).find_all('span')[1].get_text().strip())\n",
    "            info[\"degree_year\"]=[degree_year]\n",
    "    except:\n",
    "        #print('no data')\n",
    "        pass     \n",
    "    #######Endorsements\n",
    "    try:\n",
    "        en=[]\n",
    "        for i in range(len(soup.find_all('span', {'class':'custom-highlight t-bold'}))):\n",
    "            en.append(soup.find_all('span', {'class':'custom-highlight t-bold'})[i].get_text().strip())\n",
    "            info[\"endorsements\"]=[en]  \n",
    "    except:\n",
    "        #print('no data')\n",
    "        pass         \n",
    "\n",
    "    ######Skills\n",
    "    try:\n",
    "        ss=[]\n",
    "        for i in range(len(soup.find_all('div', {'class':'pv-skill-category-entity__skill-wrapper'}))):\n",
    "            ss.append(soup.find_all('div', {'class':'pv-skill-category-entity__skill-wrapper'})[i].find('p',{'class':'pv-skill-category-entity__name tooltip-container'}).get_text().strip())\n",
    "            info[\"skills\"]=[ss]\n",
    "    except:\n",
    "        print('no data')\n",
    "\n",
    "        pass\n",
    "    ######Skills(Outils et technologie)\n",
    "    try:\n",
    "        ss=[]\n",
    "        for i in range(len(soup.find_all('div', {'class':'pv-skill-category-entity__skill-wrapper'}))):\n",
    "            ss.append(soup.find_all('div', {'class':'pv-skill-category-entity__skill-wrapper'})[i].find('p',{'class':'pv-skill-category-entity__name tooltip-container'}).get_text().strip())\n",
    "            info[\"skills\"]=[ss]\n",
    "    except:\n",
    "        #print('no data')\n",
    "\n",
    "        pass     \n",
    " \n",
    "    #####Accomplishments\n",
    "    ####Language\n",
    "    try:\n",
    "        Accomp_section = soup.find('section', {'class': 'accordion-panel pv-profile-section pv-accomplishments-block languages ember-view'})        \n",
    "\n",
    "        ll=[]\n",
    "        for i in range(len(Accomp_section.find('ul').find_all('li'))):\n",
    "            ll.append((Accomp_section.find('ul').find_all('li'))[i].get_text().strip())\n",
    "            info[\"Language\"]=[ll]\n",
    "    except:\n",
    "        #print('no data')\n",
    "        pass \n",
    "    ######Projects\n",
    "    try:\n",
    "        pp=[]\n",
    "        projets_section=soup.find('section', {'class': 'accordion-panel pv-profile-section pv-accomplishments-block projects ember-view'})        \n",
    "\n",
    "        projets_section.find_all('li', {'class':\"pv-accomplishments-block__summary-list-item\"})\n",
    "        for i in range(len(projets_section.find_all('li', {'class':\"pv-accomplishments-block__summary-list-item\"}))):\n",
    "            pp.append(projets_section.find_all('li', {'class':\"pv-accomplishments-block__summary-list-item\"})[i].get_text().strip())\n",
    "            info[\"project_num\"]=[pp]\n",
    "    except:\n",
    "        #print('no data')\n",
    "        pass     \n",
    "    ######number of Projects\n",
    "    try:\n",
    "        projets_section=soup.find('section', {'class': 'accordion-panel pv-profile-section pv-accomplishments-block projects ember-view'})        \n",
    "\n",
    "        for i in projets_section.find(\"h3\").find('span', {'class':\"visually-hidden\"}).get_text().strip():        \n",
    "            try:\n",
    "                (int(i))\n",
    "                info[\"nobre de projets\"]=[int(i)]\n",
    "                #print(int(i))\n",
    "            except:\n",
    "                pass\n",
    "    except:\n",
    "        #print('no data')\n",
    "        pass     \n",
    " \n",
    "    ######Publications and numbers\n",
    "    try:\n",
    "        publication_section = soup.find('div', {'id':'publications-expandable-content'})\n",
    "        #publication=dict()\n",
    "        p=[]\n",
    "        #nombre=dict()\n",
    "        nb=0\n",
    "        \n",
    "        for i in range(len(soup.find_all('div', {'id':'publications-expandable-content'}))):\n",
    "            p.append(soup.find_all('div', {'id':'publications-expandable-content'})[i].find('li').get_text().strip())\n",
    "            info[\"Publication\"]=[p]\n",
    "            nb=nb+1\n",
    "            info[\"Pub_numbers\"]=[nb]\n",
    "\n",
    "    except:\n",
    "        #print('no data')\n",
    "        pass \n",
    "    ######Interests\n",
    "    try:\n",
    "        \n",
    "        interests_section = soup.find('section', {'class':\"pv-profile-section pv-interests-section artdeco-container-card ember-view\"})\n",
    "        interests_name=[]\n",
    "        for i in range(len(interests_section.find_all('div', {'class':\"pv-entity__summary-info ember-view\"}))):\n",
    "            inter=interests_section.find_all('div', {'class':\"pv-entity__summary-info ember-view\"})[i]\n",
    "            interests_name.append(inter.find('span').get_text().strip())\n",
    "            info[\"interests\"]=[interests_name]\n",
    "    except:\n",
    "        print('no data')\n",
    "        pass\n",
    "    ######Certifications\n",
    "    try:\n",
    "        \n",
    "        certif=[]\n",
    "        for i in range(len(soup.find_all('h3', {'class':\"t-16 t-bold\"}))):\n",
    "            certif.append(soup.find_all('h3', {'class':\"t-16 t-bold\"})[i].get_text().strip())\n",
    "            info[\"certifications\"]=[certif]\n",
    "    except:\n",
    "        #print('no data')\n",
    "        pass       \n",
    "    time.sleep(1)\n",
    "    final_liste.append(info)\n",
    "   \n",
    "    return(final_liste)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls=pd.read_csv(\"url_scrap.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def liste_final_scrapped(urls):\n",
    "    liste=[]\n",
    "    #for i in range(len(urls['url'])):\n",
    "    for i in range(7501,8000):\n",
    "        liste.append(Json_Scraping_2(urls['url'][i]+\"/\"))\n",
    "\n",
    "    return liste\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Frame_final_scrapped(urls):    \n",
    "    liste=liste_final_scrapped(urls)\n",
    "    data_liste=[]\n",
    "    for j in range(len(liste)):\n",
    "        df=pd.DataFrame(liste[j])\n",
    "        data_liste.append(df)\n",
    "    return data_liste    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_final_scrapped(urls):\n",
    "    data_liste=Frame_final_scrapped(urls)\n",
    "    data_final=pd.concat(data_liste, axis=0, ignore_index=True)\n",
    "    return data_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no data\n",
      "no data\n"
     ]
    },
    {
     "ename": "JavascriptException",
     "evalue": "Message: javascript error: Cannot read property 'scrollHeight' of null\n  (Session info: chrome=80.0.3987.149)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJavascriptException\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-2727edde2f8f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_final_scrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-22-5d17f3a4a9da>\u001b[0m in \u001b[0;36mdata_final_scrapped\u001b[1;34m(urls)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdata_final_scrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mdata_liste\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFrame_final_scrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mdata_final\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_liste\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata_final\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-594e8ce8a852>\u001b[0m in \u001b[0;36mFrame_final_scrapped\u001b[1;34m(urls)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mFrame_final_scrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mliste\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mliste_final_scrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mdata_liste\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mliste\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mliste\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-85d87142d624>\u001b[0m in \u001b[0;36mliste_final_scrapped\u001b[1;34m(urls)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m#for i in range(len(urls['url'])):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m7501\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mliste\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mJson_Scraping_2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'url'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"/\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mliste\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-26cbc01828d6>\u001b[0m in \u001b[0;36mJson_Scraping_2\u001b[1;34m(link)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mJson_Scraping_2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0msoup\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mHTML_Sraping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mfinal_liste\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0minfo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0minfo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"link\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-44c091cce99c>\u001b[0m in \u001b[0;36mHTML_Sraping\u001b[1;34m(link)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0my1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# Get scroll height\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute_script\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"return document.body.scrollHeight\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;31m#pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute_script\u001b[1;34m(self, script, *args)\u001b[0m\n\u001b[0;32m    634\u001b[0m         return self.execute(command, {\n\u001b[0;32m    635\u001b[0m             \u001b[1;34m'script'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mscript\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 636\u001b[1;33m             'args': converted_args})['value']\n\u001b[0m\u001b[0;32m    637\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexecute_async_script\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscript\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[0;32m    323\u001b[0m                 response.get('value', None))\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    240\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'alert'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mJavascriptException\u001b[0m: Message: javascript error: Cannot read property 'scrollHeight' of null\n  (Session info: chrome=80.0.3987.149)\n"
     ]
    }
   ],
   "source": [
    "data=data_final_scrapped(urls)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
